---
title:  "[Artificial Intelligence] 이전 손실을 이용하는 모멘텀"

categories:
  - AI
tags:
  - AI
  - 인공지능
  - 모멘텀
  - Momentum
  - 옵티마이저
  - Optimizer
  - 확률적 경사하강법
  - SGD
  - 아담
  - ADAM
  - 손실
  - Loss
  - 손실함수
  - Loss Function

date: 2024-11-12

editLink: false
lastUpdated: true
---

## 모멘텀(Momentum)은 무엇인가?
`모멘텀(Momentum)`은 물리학에서 사용되는 용어와 비슷한 개념이라고 보면 될 것 같습니다.  
물리학에서는 운동량, 물체가 특정 이동하려고 하는 것을 의미하고 이는 `관성`이라 보면 됩니다.

### 모멘텀의 고안 이유
`모멘텀`은 [기존 포스팅](https://blog.false.kr/posts/Computing/AI/SGD-Minibatch.html)에서 설명드린 `확률적 경사하강법(SGD)`의 단점을 보완하기 위한 요소입니다.  

기존의 `SGD`를 이해하신 경우 가중치 갱신 간 `발산` 등의 `노이즈`로 인해 속도가 느려질 수 있습니다.  
속도 문제는 `데이터`의 `차원`과 `양`이 증가하면서 `시간`적인 부분과 `정확성`에 대한 문제가 생깁니다.

이러한 한계를 극복하기 위해 고안된 것이 `모멘텀`으로 위에서 설명한 `관성`의 원리를 이용하게 됩니다.

### 모멘텀의 동작 원리
`관성`을 생각해보면 이전에 발생한 `운동량`이 현재의 `운동량`에 영향을 끼치는 것이라 볼 수 있습니다.  

`모멘텀`은 이전에 `세대`에서 갱신된 `가중치`와 `방향`을 이용하여 현재 `세대`에 일부 적용하는 것입니다.  
그렇게 함으로써 `SGD`에서 발생되는 `발산`으로 인한 `노이즈`를 줄이는 역할을 수행할 수 있습니다. 

기존의 `SGD`와 비교하여 어떠한 양상의 차이가 존재하는지 그림으로 한 번 알아보도록 하겠습니다.

