---
title:  "[Artificial Intelligence] 확률적 경사하강법과 미니 배치"

categories:
  - AI
tags:
  - AI
  - 인공지능
  - 경사하강법
  - Gradient Descent
  - 확률적 경사하강법
  - SGD
  - Stochastic Gradient Descent
  - 배치 모드
  - 배치
  - Batch
  - Batch Gradient Descent
  - 패턴 모드
  - 패턴
  - Pattern
  - 미니 배치
  - Mini-Batch
  - Mini-Batch Gradient Descent

date: 2024-11-04

editLink: false
lastUpdated: true
---
::: info
📢 2024년 2학기 `딥러닝` 교과 과정 중 미니 배치의 동작 구조에 대해 궁금해졌습니다.  
조사한 내용을 작성한 것으로 틀린 내용이나 추가 내용이 있으면 댓글 부탁드립니다!
:::

## 확률적 경사하강법 (Stochastic Gradient Descent)
[이전 포스팅](/posts/Computing/AI/Gradient-descent)에서 `경사하강법`에 대해 알아봤고 문제점에 대해서도 알아봤습니다.

이를 보완한 `확률적(Stochastic) 경사하강법`은 무엇이고 어떤 개념이 있는지 알아봅니다.

우선 이를 알기 위해 `경사하강법`에서 사용하는 `가중치 갱신`에 대한 모드를 알아봐야 합니다.  
대표적인 모드로는 `배치(Batch)`, `패턴(Pattern)`, `미니 배치(Mini-Batch)`가 존재합니다.

### 배치 모드 (Batch)
`IT 업계`에 관심있으신 분들은 `Batch`라는 단어를 들어봤을 것이라 생각합니다.  
`Batch`는 영어를 직역하면 `일괄`이라는 뜻으로 `일괄 처리 작업`을 일컫고 있습니다.

`Batch` 작업은 보고서 생성과 같이 `정해진 시간에 한 번에 처리되는 일`을 의미합니다.  
`경사하강법`에서의 `배치 모드`도 다르지 않으며 동작되는 원리는 아래와 같이 진행됩니다

`기계학습`에서는 전체 `학습 데이터`를 `한 번` 전방/역전파 계산한 것을 `Epoch`라 칭합니다.  
`Epoch`(에포크)는 `세대`를 뜻하고 `1세대에 대한 학습이 완료`됐다는 것도 동일 표현입니다.

`기계학습`은 `데이터`의 관계 파악 및 `가중치 갱신`을 위해 여러 번의 `Epoch`를 수행하게됩니다.  
(예를 들어 "`Epoch`를 100번 수행했다"는 것은 100번의 `반복 학습`을 수행했다는 것입니다.)

여기서 `가중치 갱신`을 `배치 모드`에서는 `1 Epoch`에 한 번만 전체적으로 반영하게 됩니다.  

`배치 모드`는 `시각화`하여 표현할 경우 `가중치`를 아래와 같이 갱신하게 됩니다.

![](/assets/image/Post/Computing/AI/SGD-Minibatch/1.png "배치 모드" =70%x70%)

#### 배치 모드의 특징
`배치 모드`를 사용했을 때의 특징은 대표적으로 3가지가 있다고 볼 수 있습니다.

1. `전체 데이터`를 이용하여 학습하기에 후술할 모드 중 `가장 갱신 횟수가 적다는 것`
2. `전체 데이터`에 대한 오차 등을 모두 갖고 있어야 하기에 `메모리 사용량이 높다는 것`
3. `전체 데이터`를 기반으로 변함없이 이용하기에 `발산`보다는 `수렴`하는 형태

#### 배치 모드의 문제점
기본적으로 `경사하강법`의 방식이 `배치 모드`라고 이해해도 문제가 없을 것 같습니다.  

1. [이전 포스팅](/posts/Computing/AI/Gradient-descent)에서 서술한 것과 같이 `Local Minimum`으로 빠질 확률이 높다는 문제
2. `가중치 갱신`이 적기에 정확도의 이슈가 발생될 수 있다는 문제 (가중치 학습의 속도 문제)
3. `전체 데이터`가 큰 경우에는 `메모리 사용량`의 부담이 존재

### 패턴 모드 (Pattern)
`패턴 모드`는 각 `데이터`마다 `학습`을 진행한 이후 `가중치`를 갱신하는 방식입니다.  
`확률적 경사하강법`은 `패턴 모드` 자체 또는 `미니 배치`를 포함하여 표현하게 됩니다.

간단하게 말해 `전체 데이터` 하나하나에 대해 학습 간 `가중치`를 갱신하는 것입니다.  
이때 `데이터`의 순서를 `Epoch`마다 다르게 뒤섞어 `랜덤 샘플링`의 효과를 주게 됩니다.

이렇게 수행할 경우 `배치 모드`와 달리 데이터의 `손실` 등의 계산에 차이가 발생됩니다.

또한 `가중치 갱신` 횟수도 기존 `배치 모드`의 경우 `1 Epoch` 당 1회만 수행하였다면  
`패턴 모드`는 `1 Epoch` 당 `전체 데이터의 개수`만큼 `가중치 갱신`이 발생됩니다.

`데이터`가 1,000개고, `Epoch`가 50번이라면 총 50,000번의 `가중치 갱신`이 발생됩니다.

`패턴 모드`는 `시각화`하여 표현할 경우 `가중치`를 아래와 같이 갱신하게 됩니다.

![](/assets/image/Post/Computing/AI/SGD-Minibatch/2.png "패턴 모드" =70%x70%)

#### 패턴 모드의 특징
`패턴 모드`의 가장 큰 특징으로는 아래와 같은 3가지 사항이 존재합니다.

1. 적은 데이터로도 많은 `가중치 갱신`을 통한 학습 이점을 가져올 수 있습니다.
2. `랜덤 샘플링` 효과로 `Local Minimum`에 빠질 확률을 줄일 수 있습니다.
3. `데이터`마다 수행되므로 `가중치 갱신`의 메모리 부담과 연산량이 작습니다.

#### 패턴 모드의 문제점
결국 `패턴 모드`는 `전체 데이터`에 대해 각각의 `가중치 갱신` 작업을 수행하게 됩니다.  
이로 인해 발생되는 `문제점`은 아래와 같고 최근에는 `미니 배치`를 사용하고 있습니다.

1. `가중치 갱신`의 부담은 적지만 `1 Epoch`의 소요 시간이 오래 걸립니다.
2. `랜덤샘플링`으로 인한 `발산`으로 `Global Minimum`에 수렴하기는 어렵습니다.
3. 1개의 `데이터`마다 `가중치 갱신`이 이뤄지므로 `병렬` 처리에 어려움이 있습니다.

시간은 오래 소요되나 트렌드인 `병렬` 처리를 이용하기 어렵다는 점이 문제점으로 보입니다.

### 미니 배치 (Mini-Batch)
`미니 배치`는 `배치 모드`와 `패턴 모드`의 장점을 받아들여 중간점에 있는 방식입니다.  
`미니배치 경사하강법`이라 칭하기도 하고 `확률적 경사하강법`에 포함하기도 합니다.

`미니 배치`는 `데이터`를 일정 크기의 `부분 집합`으로 나눠 `학습` 후 `가중치`를 갱신합니다.  

`부분 집합`의 크기를 지정하는 매개변수를 `배치 크기(Batch size)`라 칭하고 있으며,  
데이터를 `부분 집합`으로 분할할 때 `랜덤 샘플링`을 적용하여 분할하게 됩니다.

예를 들어 `학습 데이터`가 1,000개 이고 `배치 크기` 매개변수를 100으로 지정한 경우  
`학습 데이터`를 100개씩 보유한 10개의 `배치(Batch)`를 이용하여 학습을 수행합니다.

10개의 `배치`는 `1 Epoch` 안에서 순차적으로 `학습` 후 `가중치 갱신` 작업을 수행합니다.  
이 경우 `1 Epoch` 안에서 10번의 `가중치 갱신`이 이뤄진 것을 확인할 수 있습니다.

이렇게 `가중치 갱신`을 수행하는 횟수를 `이터레이션(Iteration)`이라 칭합니다.

`미니 배치`는 `시각화`하여 표현할 경우 `가중치`를 아래와 같이 갱신하게 됩니다.

![](/assets/image/Post/Computing/AI/SGD-Minibatch/3.png "미니 배치" =70%x70%)

#### 미니 배치의 특징
`미니 배치`의 특징으로는 아래와 같은 사항이 있다고 볼 수 있습니다.

1. `패턴 모드`를 `부분 집합` 단위로 수행하여 `배치 모드`와 중간점을 이룹니다.
2. `패턴 모드`의 `발산`과 `시간` 부담을 줄이고 `집합` 별 `병렬` 처리가 가능합니다.
3. `배치 모드`의 `정확성` 문제와 `메모리` 부담을 줄일 수 있다는 점입니다.

#### 미니 배치의 문제점
`미니 배치`도 문제점은 다수 존재하며 아래와 같은 문제가 있습니다.

1. `데이터`가 `배치 크기`에 맞지 않는 경우 마지막 `배치`의 영향력 상승
2. `배치 크기`에 따른 학습 성능의 차이가 있기에 적당한 값을 찾는 것이 중요

2번의 경우 결국 `배치 크기`가 너무 작은 경우 `패턴 모드`의 문제점을 일으키고  
반대로 너무 큰 경우에는 `배치 모드`의 문제점에 가까워진다는 것을 알 수 있습니다.

### 확률적이라는 표현을 사용하는 이유?
`확률적 경사하강법`은 결국 `패턴 모드` 자체 또는 `미니 배치`까지를 의미합니다.  
`확률적`이라는 표현은 각 모드에서의 `랜덤샘플링`이 적용됐다는 것에 있습니다.

`랜덤샘플링`이 적용됐기에 `Local Minimum`을 피하는 데에 도움을 줄 수 있습니다.

## 용어 정리
`세대(Epoch)`: `기계학습`이 수행될 때 반복 학습의 `횟수`를 일컫는 말  
`배치 크기(Batch size)`: `미니 배치`에서 1개의 `부분 집합`에 포함될 `데이터`의 개수  
`이터레이션(Iteration)`: `배치 크기`로 나눠진 `부분 집합`의 개수 (or `가중치 갱신` 횟수)
- - -

끝까지 읽어주셔서 감사드립니다. 😎