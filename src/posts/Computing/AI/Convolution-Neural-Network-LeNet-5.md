---
title:  "[Artificial Intelligence] LeNet-5을 통해 알아보는 합성곱 신경망"

categories:
  - AI
tags:
  - AI
  - 인공지능
  - 합성곱
  - Convolution
  - 합성곱 신경망
  - Convolution Neural Network
  - 신경망
  - Neural Network
  - 레이어
  - 층
  - Layer
  - 커널
  - Kernel
  - 필터
  - Filter
  - 특징 맵
  - Feature Map
  - 패딩
  - Padding
  - 스트라이드
  - Stride
  - 채널
  - Channel
  - Flat
  - Flatten
  - 평면화
  - 이미지 인식
  - 객체 인식
  - Object Detection
  - LeNet-5
  - MNIST
  - 손글씨

date: 2024-11-29

order: 302
editLink: false
lastUpdated: true
---
## 📚 합성곱 신경망의 구조
[이전 포스팅](/posts/Computing/AI/Convolution-Neural-Network-Intro)에서는 `합성곱 신경망`을 구성하기 위한 기초 사항에 대해서 알아보았습니다.  

`합성곱 신경망`도 마찬가지로 `깊은 신경망`으로 구성하고 있어 구조 파악이 필요합니다.  
만약 여러 `층`으로 구성된다면 각 층의 `특징 맵`의 `크기`와 `채널`을 미리 파악해야 합니다.

마치 빌딩 블록처럼 `신경망`을 쌓아가고 어떠한 형식으로 쌓아가는지 확인해보겠습니다.

### 📕 구조 확인
![](/assets/image/Post/Computing/AI/Convolution-Neural-Network-LeNet-5/1.png "CNN의 예제 구조" =80%x80%)

`합성곱 신경망`은 위와 같이 간단하게 쌓아놓은 예제로 상위와 같이 구성될 수 있습니다.  
해당 구성은 간단한 표현을 위한 구성도이나 확인해보면 일정한 특징 확인이 가능합니다.

우선 단일 `커널`의 면의 수, 즉 `채널`은 `입력 텐서(데이터)`의 `채널`과 동일하다는 것과  
`커널`의 개수(`k'`)가 다음 층의 `특징 맵`의 `채널`과 동일하다는 것이 확인되고 있습니다.

이런 특징은 `합성곱 신경망`에서 지속 사용되는 개념이니 미리 이해하는 것이 좋습니다.

## 💫 LeNet-5
`합성곱 신경망`의 구조를 이용하여 필기 숫자인식 문제에서 입증한 알고리즘입니다.

해당 알고리즘은 지금의 `합성곱 신경망`을 이용하는 방식에 비해 단순한 구조이지만,  
`합성곱 신경망`의 기본적인 구조를 이해하는데 도움이 될 것이라 생각하여 소개합니다.

![](/assets/image/Post/Computing/AI/Convolution-Neural-Network-LeNet-5/2.png "LeNet-5 구조 ([`LeCun1998`](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf))" =80%x80%)

첨부해놓은 그림은 `LeNet-5`가 공개된 [`LeCun1998`](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)이라는 논문의 첨부된 그림입니다.

해당 그림에서는 `Convolutions`(`합성곱`) 과정과 `Subsampling`로 표현된 `풀링` 과정,  
`Full connection`인 `완전연결층`, 결과 출력의 `Gaussian connections`가 있습니다.

각 `층`을 지나면서 이미지에 대한 `데이터 크기`와 `채널`의 변경점을 표현하고 있지만,  
이를 조금 더 보완하여 각 층마다 어떠한 `파라미터`를 갖고 있는지 알아보겠습니다.

![](/assets/image/Post/Computing/AI/Convolution-Neural-Network-LeNet-5/3.png "LeNet-5 해석" =90%x90%)

각 `합성곱 층`과 `풀링 층`에서 `파라미터`와 마지막에 `Flat`해진 것이 확인되고 있습니다.  
이렇게 `Flat`해진 데이터를 `완전연결층`을 통해 점차 줄이고 10개의 `출력`으로 축소됩니다.

`Flat`의 과정을 `Flatten`이라고 표현하며 `완전연결층`에서의 `가중치`를 줄이기 위해서  
최근에는 `전역 평균 풀링`을 층으로 만들어 `Flatten` 과정을 수행하기도 하고 있습니다.

이러한 과정에서 사용된 `가중치`의 개수를 계산할 수 있는데 아래와 같이 계산되게 됩니다.

- 가중치 개수
  - 합성곱 층: (5\*5\*1+1)\*6 + (5\*5\*6+1)*16 + (5\*5\*16+1)\*120 = 50,692 개
  - 완전연결층: (120+1)\*84 + (84+1)\*10 = 11,014 개
  - 총합: 50,692 + 11,014 = 61,706 개

그렇다면 총 갱신해야하는 `가중치`의 개수는 61,706개라는 것을 확인할 수 있을 겁니다.

## 합성곱 신경망의 특징과 장단점
### ➡️ 손실 함수와 최적화 함수
이전 `MSE`나 [`엔트로피`](/posts/Computing/AI/Cross-entropy)와 같은 `손실 함수`와 [`경사하강법`](/posts/Computing/AI/Gradient-descent)과 같은 `최적화 함수`의 개념이  
동일하게 적용되며, `출력층`에 `노드`는 `softmax`와 같은 함수로 결과를 표현하게 됩니다.

기존 `완전연결층`과 차이가 있다면 `노드` 간 연결되는 `엣지`에 대한 `가중치`를 갱신했다면  
`합성곱 신경망`은 `커널`에 존재하는 공용으로 사용되는 `가중치`가 갱신된다는 것 입니다.

이렇게 된다면 각각의 `데이터`, `노드`가 한 층에서는 같은 `가중치`를 공유하게 됩니다.  
이를 통해 `완전연결층`의 노드 간 `가중치`에 비해 갱신이 필요한 가중치가 적어집니다.

### 🪣 통째 학습
기존의 `컴퓨터 비전`에서는 [지난 포스팅](/posts/Computing/AI/Convolution-Neural-Network-Intro#🔢-특징-맵-feature-map)에 표현과 같이 `수직 엣지`와 `수평 엣지` 등  
구조를 사전 정의하고 파악 후 `커널`을 사람이 직접 설계해야하는 문제가 있었습니다.

현재는 `딥러닝`의 `역전파` 등의 `가중치` 갱신 방법을 이용하여 `특징 추출`을 진행 후  
이를 `분류`까지 진행 후 돌아와 `가중치`를 갱신하는 형식을 이용하기 시작했습니다.

이렇게 된다면 `특징` 파악부터 `분류`까지 모두 한 번에 진행하는 것을 알 수 있으며,  
`특징`을 학습한다는 `특징 학습` 또는 모두 진행의 의미로 `통째 학습`이라 표현합니다.

### 🤔 장점
- `통째 학습`
  - 이전 소개한 것처럼 `특징 추출`부터 `분류`까지 모든 과정을 제공하고 있습니다.
- `특징 학습`
  - `특징 추출`에 대한 알고리즘을 `학습`을 통해 파악하기에 성능 향상이 있습니다.
- `신경망`의 깊이
  - `가중치 공유`를 이용하기 때문에 깊이가 깊어도 `학습`의 영향도가 적습니다.
  - 소개한 `LeNet-5` 이후 `신경망`은 수십, 수백 층의 깊이로 구성하고 있습니다.
- `데이터` 구조 유지
  - 컬러 이미지의 경우 `3차원 텐서`로 `합성곱 층` 사용 시 `3차원 텐서`를 출력합니다.
  - `다층 퍼셉트론`의 경우 `1차원 벡터` 형태로 제공하고 있기에 정보 손실이 있습니다.
- `부분 연결성`
  - 기존의 `다층 퍼셉트론`은 `완전 연결성`의 특성으로 모든 `노드`가 `엣지`로 연결됩니다.
  - 반면에 `합성곱 신경망`은 `커널`, `패딩`, `스트라이드`의 구조로 `부분 연결성`을 갖습니다.
- `가중치 공유`
  - `커널` 구조를 이용하기에 `가중치 공유` 한 층에서의 `가중치 갱신` 대상이 적습니다.

### 😥 단점
- `가중치 연산 수`
  - `합성곱 신경망`의 특성 상 중복되는 영역의 `커널`을 적용하는 경우가 발생됩니다.  
  이러한 경우 연산의 수가 폭발적으로 증가한다는 것을 파악하실 수 있을겁니다.
    - 예를 들어 5\*5의 `커널`을 이용하여 32\*32 이미지를 6개의 `채널`로 만들 때  
      5 \* 5 \* 28 \* 28 \* 6 = 117,600 번의 `가중치` 연산을 수행하게 됩니다.
  - `합성곱 신경망`을 이용하면 기존의 연산에 비해 속도가 느리다는 것을 의미합니다.

- - -

다음은 `합성곱 신경망` 중 다른 `신경망`의 구조에 대해 알아보도록 하겠습니다.

끝까지 읽어주셔서 감사드립니다. 😎