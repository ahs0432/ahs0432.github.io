import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,o as t,a as o}from"./app-DyM4jzy7.js";const d="/assets/image/Post/Computing/AI/Adaptive-learning-rate/1.png",r="/assets/image/Post/Computing/AI/Adaptive-learning-rate/2.png",c="/assets/image/Post/Computing/AI/Adaptive-learning-rate/3.png",i="/assets/image/Post/Computing/AI/Adaptive-learning-rate/4.png",n={},p=o('<h2 id="😀-적응적-학습률-adaptive-learning-rate-의-개념" tabindex="-1"><a class="header-anchor" href="#😀-적응적-학습률-adaptive-learning-rate-의-개념"><span>😀 적응적 학습률(Adaptive learning rate)의 개념</span></a></h2><p><a href="/posts/Computing/AI/Gradient-descent"><code>경사하강법</code></a>에서 소개한 내용을 토대로 확인 시 <code>𝜌(rho)</code>라 불리는 <code>학습률</code> 값의 경우<br><code>가중치</code> 갱신 과정에서 모두 동일한 값을 이용하여 수행된다는 것을 확인할 수 있습니다.</p><p>이를 동일 값을 사용하는 것이 아닌 각 <code>매개변수</code> 별 다른 값을 이용하는 것이 고안됐고,<br> 이러한 방법을 <code>적응적 학습률(Adaptive learning rate)</code>이라 표현하고 있습니다.</p><h3 id="😏-adagrad-adaptive-gradient" tabindex="-1"><a class="header-anchor" href="#😏-adagrad-adaptive-gradient"><span>😏 AdaGrad (Adaptive Gradient)</span></a></h3><h4 id="🤔-개념" tabindex="-1"><a class="header-anchor" href="#🤔-개념"><span>🤔 개념</span></a></h4><p><code>적응적 학습률</code>의 개념을 처음으로 도입한 <code>Optimizer</code>로 볼 수 있는 알고리즘입니다.<br> 이전 <code>그레이디언트(기울기)</code>를 누적한 벡터를 이용하여 학습률을 적응적으로 설정합니다.</p><p>방식을 간단하게 보자면 <code>학습률</code>이 <code>매개변수</code>가 업데이트됨에 따라 반비례로 작아집니다.</p><p><code>매개변수</code> 별 모두 다른 값을 이용하기에 <code>갱신</code>된 값이 큰 요소의 경우 <code>작은 학습률</code>을 갖고<br> 반대로 <code>갱신</code>된 값이 작은 요소의 경우 반대로 <code>큰 학습률</code>을 갖게하는 방식으로 동작합니다.</p><p>이를 통해 <code>학습</code>이 진행되는 과정에서 <code>가중치 갱신</code> 간 <code>학습률</code>이 적응적으로 변동됩니다.</p><h4 id="🦾-동작-방법" tabindex="-1"><a class="header-anchor" href="#🦾-동작-방법"><span>🦾 동작 방법</span></a></h4><p><code>AdaGrad</code>는 아래와 같은 알고리즘과 수식을 이용해 동작되는 것을 확인할 수 있습니다.</p><figure><img src="'+d+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>AdaGrad 알고리즘</figcaption></figure><p>알고리즘을 간단하게 리뷰하면 <code>𝜌(학습률)</code>을 이용하여 <code>손실</code>의 <code>기울기</code>를 구하고<br> 각 요소 별 <code>기울기</code>의 제곱 값을 구하고 기존의 <code>r(누적 벡터)</code> 값에 덧셈 연산합니다.</p><p>구해진 <code>r</code> 값의 <code>제곱근</code>을 분모로 두고 <code>𝜌</code>와 각 <code>기울기</code>를 곱한 값을 분자로 두어<br> 각 <code>매개변수</code> 별 갱신할 <code>가중치</code> 값을 도출한 뒤 반영하는 것을 반복하는 구조입니다.<br> (여기서 <code>𝜖</code>는 분모가 0에 수렴되지 않도록 하기 위함으로 매우 작은 값으로 구성)</p><p>알고리즘 상에 표기된 5~7번의 경우 수식을 풀어볼 경우 아래와 같이 구성됩니다.</p><figure><img src="'+r+'" alt="" width="50%" height="50%" tabindex="0" loading="lazy"><figcaption>AdaGrad 수식 해석</figcaption></figure><p>5번 수식의 경우 각 요소 별 기존 <code>r</code> 값에 <code>g</code>의 제곱의 합을 구하는 것을 의미합니다.</p><p>6번 수식은 <code>𝜌</code>와 <code>기울기</code>를 곱한 값을 <code>r</code> 제곱근으로 나누어 갱신 <code>가중치</code>를 구합니다.<br> 6번 수식에서 <code>기울기 합</code>에 따라 <code>학습률</code> 값이 변동되어 적용됨을 확인할 수 있습니다.</p><p>마지막으로 7번 수식은 <code>가중치</code>에 갱신 <code>가중치</code>를 더하여 다음 <code>가중치</code>를 구한 것입니다.</p><h4 id="😅-문제점" tabindex="-1"><a class="header-anchor" href="#😅-문제점"><span>😅 문제점</span></a></h4><p><code>AdaGrad</code>는 초기 학습에서는 좋은 성능을 보이지만 <code>학습</code>이 진행될수록 문제가 있습니다.<br> 여기서 발생되는 문제는 보통 2가지 정도로 나뉠 수 있으며 아래와 같이 정리 가능합니다.</p><ol><li><code>학습</code> 지속에 따른 <code>r</code>의 값의 증가로 인한 <code>학습률</code>의 값 감소로 인한 학습 지연</li><li>학습 기간에 따른 <code>그레이디언트</code> 비중이 동일하게 반영되어 최신성 반영의 문제</li></ol><h3 id="😲-rmsprop-root-mean-square-propagation" tabindex="-1"><a class="header-anchor" href="#😲-rmsprop-root-mean-square-propagation"><span>😲 RMSProp (Root Mean Square Propagation)</span></a></h3><h4 id="🤔-개념-1" tabindex="-1"><a class="header-anchor" href="#🤔-개념-1"><span>🤔 개념</span></a></h4><p><code>AdaGrad</code>의 단점인 <code>학습 지연 문제</code>와 <code>그레이디언트</code>의 비중 역할 문제 해소를 위해<br> 이전 <code>그레이디언트</code>를 누적할 때 오래된 것의 영향을 줄이는 정책을 적용한 방법입니다.</p><h4 id="🦾-동작-방법-1" tabindex="-1"><a class="header-anchor" href="#🦾-동작-방법-1"><span>🦾 동작 방법</span></a></h4><p><code>RMSProp</code>에서는 <code>a</code>라는 값을 이용하여 <code>가중 이동 평균 기법</code>을 방법이라 볼 수 있습니다.</p><p>동작하는 방식의 알고리즘은 아래과 같이 구성되고 <code>수식</code>은 <code>AdaGrad</code>와 거의 동일합니다.</p><figure><img src="'+c+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>RMSProp 알고리즘</figcaption></figure><p>알고리즘 상 5번 부분에 존재하지 않던 <code>a</code> 값이 위치한 것을 확인할 수 있습니다.<br><code>a</code> 값이 작을수록 최신 <code>그레이디언트</code>에 비중을 두어 <code>가중치</code> 갱신을 수행합니다.</p><p>일반적으로 <code>0.9</code>, <code>0.99</code>, <code>0.999</code> 값을 사용하는 것으로 알려져 있습니다.</p><p>변경된 <code>수식</code>인 5번에 대한 내용만 수식으로 따로 정리해보면 아래와 같습니다.</p><figure><img src="'+i+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>RMSProp 수식 해석</figcaption></figure><p>위와 같이 동작하게 된다면 기존의 <code>기울기 벡터</code>는 학습 과정에서 점점 작아집니다.<br> 또한 값이 커져서 발생되는 문제도 <code>기울기</code>를 더할 때 값을 축소하기에 약화됩니다.</p><h4 id="😅-문제점-1" tabindex="-1"><a class="header-anchor" href="#😅-문제점-1"><span>😅 문제점</span></a></h4><p><code>AdaGrad</code>에 대한 한계를 어느정도 극복하여 만들어진 <code>RMSProp</code>도 문제점은 존재합니다.<br> 문제는 아래처럼 총 3가지 정도로 정리되오니 참고하시어 이용해주시면 될 것 같습니다.</p><ol><li><code>a</code>와 같은 <code>하이퍼파라미터</code> 설정이 늘어나고 민감도 상승</li><li>모멘텀을 사용하지 않기에 <code>방향성</code>을 찾아가는 속도의 한계</li><li>이전 <code>기울기</code>를 축소에 따른 <code>기울기 소실</code> 현상 악화 가능성</li></ol><p>위와 같은 문제로 인해 <code>Adam</code>이라는 <code>Optimizer</code>가 고안됐고 다음 포스팅으로 소개하겠습니다.</p><hr><p>끝까지 읽어주셔서 감사드립니다. 😎</p>',40),s=[p];function l(g,m){return t(),a("div",null,s)}const u=e(n,[["render",l],["__file","Adaptive-learning-rate.html.vue"]]),f=JSON.parse('{"path":"/posts/Computing/AI/Adaptive-learning-rate.html","title":"[Artificial Intelligence] 학습률을 조정하는 적응적 학습률","lang":"ko-KR","frontmatter":{"title":"[Artificial Intelligence] 학습률을 조정하는 적응적 학습률","categories":["AI"],"tags":["AI","인공지능","적응적 학습률","Adaptive learning rate","AdaGrad","Adaptive Gradient","RMSProp","Root Mean Square Propagation","Optimizer","확률적 경사하강법","SGD","손실","Loss","손실함수","Loss Function"],"date":"2024-11-17T00:00:00.000Z","order":205,"editLink":false,"lastUpdated":true,"description":"😀 적응적 학습률(Adaptive learning rate)의 개념 경사하강법에서 소개한 내용을 토대로 확인 시 𝜌(rho)라 불리는 학습률 값의 경우 가중치 갱신 과정에서 모두 동일한 값을 이용하여 수행된다는 것을 확인할 수 있습니다. 이를 동일 값을 사용하는 것이 아닌 각 매개변수 별 다른 값을 이용하는 것이...","head":[["meta",{"property":"og:url","content":"https://blog.false.kr/posts/Computing/AI/Adaptive-learning-rate.html"}],["meta",{"property":"og:site_name","content":"찬스의 개발 블로그 : Chance Devlog"}],["meta",{"property":"og:title","content":"[Artificial Intelligence] 학습률을 조정하는 적응적 학습률"}],["meta",{"property":"og:description","content":"😀 적응적 학습률(Adaptive learning rate)의 개념 경사하강법에서 소개한 내용을 토대로 확인 시 𝜌(rho)라 불리는 학습률 값의 경우 가중치 갱신 과정에서 모두 동일한 값을 이용하여 수행된다는 것을 확인할 수 있습니다. 이를 동일 값을 사용하는 것이 아닌 각 매개변수 별 다른 값을 이용하는 것이..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://blog.false.kr/assets/image/Post/Computing/AI/Adaptive-learning-rate/1.png \\"AdaGrad 알고리즘\\" =70%x70%"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"property":"og:updated_time","content":"2024-11-17T12:29:46.000Z"}],["meta",{"property":"article:author","content":"Chance"}],["meta",{"property":"article:tag","content":"AI"}],["meta",{"property":"article:tag","content":"인공지능"}],["meta",{"property":"article:tag","content":"적응적 학습률"}],["meta",{"property":"article:tag","content":"Adaptive learning rate"}],["meta",{"property":"article:tag","content":"AdaGrad"}],["meta",{"property":"article:tag","content":"Adaptive Gradient"}],["meta",{"property":"article:tag","content":"RMSProp"}],["meta",{"property":"article:tag","content":"Root Mean Square Propagation"}],["meta",{"property":"article:tag","content":"Optimizer"}],["meta",{"property":"article:tag","content":"확률적 경사하강법"}],["meta",{"property":"article:tag","content":"SGD"}],["meta",{"property":"article:tag","content":"손실"}],["meta",{"property":"article:tag","content":"Loss"}],["meta",{"property":"article:tag","content":"손실함수"}],["meta",{"property":"article:tag","content":"Loss Function"}],["meta",{"property":"article:published_time","content":"2024-11-17T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-17T12:29:46.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"[Artificial Intelligence] 학습률을 조정하는 적응적 학습률\\",\\"image\\":[\\"https://blog.false.kr/assets/image/Post/Computing/AI/Adaptive-learning-rate/1.png \\\\\\"AdaGrad 알고리즘\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Adaptive-learning-rate/2.png \\\\\\"AdaGrad 수식 해석\\\\\\" =50%x50%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Adaptive-learning-rate/3.png \\\\\\"RMSProp 알고리즘\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Adaptive-learning-rate/4.png \\\\\\"RMSProp 수식 해석\\\\\\" =70%x70%\\"],\\"datePublished\\":\\"2024-11-17T00:00:00.000Z\\",\\"dateModified\\":\\"2024-11-17T12:29:46.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Chance\\",\\"url\\":\\"https://blog.false.kr\\",\\"email\\":\\"chance0432@naver.com\\"}]}"]]},"headers":[{"level":2,"title":"😀 적응적 학습률(Adaptive learning rate)의 개념","slug":"😀-적응적-학습률-adaptive-learning-rate-의-개념","link":"#😀-적응적-학습률-adaptive-learning-rate-의-개념","children":[{"level":3,"title":"😏 AdaGrad (Adaptive Gradient)","slug":"😏-adagrad-adaptive-gradient","link":"#😏-adagrad-adaptive-gradient","children":[]},{"level":3,"title":"😲 RMSProp (Root Mean Square Propagation)","slug":"😲-rmsprop-root-mean-square-propagation","link":"#😲-rmsprop-root-mean-square-propagation","children":[]}]}],"git":{"createdTime":1731808456000,"updatedTime":1731846586000,"contributors":[{"name":"Chance","email":"ahs0432@naver.com","commits":5},{"name":"ahs0432","email":"ahs0432@naver.com","commits":1}]},"readingTime":{"minutes":0.48,"words":143},"filePathRelative":"posts/Computing/AI/Adaptive-learning-rate.md","localizedDate":"2024년 11월 17일","excerpt":"<h2>😀 적응적 학습률(Adaptive learning rate)의 개념</h2>\\n<p><a href=\\"/posts/Computing/AI/Gradient-descent\\"><code>경사하강법</code></a>에서 소개한 내용을 토대로 확인 시 <code>𝜌(rho)</code>라 불리는 <code>학습률</code> 값의 경우<br>\\n<code>가중치</code> 갱신 과정에서 모두 동일한 값을 이용하여 수행된다는 것을 확인할 수 있습니다.</p>\\n<p>이를 동일 값을 사용하는 것이 아닌 각 <code>매개변수</code> 별 다른 값을 이용하는 것이 고안됐고,<br>\\n이러한 방법을 <code>적응적 학습률(Adaptive learning rate)</code>이라 표현하고 있습니다.</p>","autoDesc":true}');export{u as comp,f as data};
