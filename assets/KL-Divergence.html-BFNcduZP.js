import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,o as c,a as o}from"./app-eXCOKstw.js";const r="/assets/image/Post/Computing/AI/KL-Divergence/1.png",n="/assets/image/Post/Computing/AI/KL-Divergence/2.png",a={},i=o('<h2 id="📈-kl-kullback-leibler-divergence" tabindex="-1"><a class="header-anchor" href="#📈-kl-kullback-leibler-divergence"><span>📈 KL(Kullback-Leibler) Divergence</span></a></h2><p><a href="/posts/Computing/AI/Cross-entropy">이전 포스팅</a>을 통해 <code>불확실성</code>을 측정하기 위한 <code>엔트로피</code>에 대한 개념을 알아봤습니다.</p><p><code>KL-Divergence</code>는 <code>교차 엔트로피</code>를 이용하여 <code>확률분포</code>의 차이를 계산하는 함수로<br> 근사하는 다른 <code>분포</code>를 샘플링 시 발생할 수 있는 <code>정보 엔트로피</code>의 차이를 계산합니다.</p><p>여기서 계산된 것을 <code>상대 엔트로피</code>, <code>정보 획득량</code>, <code>인포메이션 다이버전스</code>라 합니다.</p><p><code>Divergence</code>는 <code>발산</code>을 의미하기 때문에 <code>분포</code>에 대한 <code>발산</code> 정도로도 볼 수 있습니다.</p><h3 id="🔢-수식-유도" tabindex="-1"><a class="header-anchor" href="#🔢-수식-유도"><span>🔢 수식 유도</span></a></h3><p><code>KL-Divergence</code>의 경우 수식을 통해 유도할 수 있는데 먼저 수식을 알아보겠습니다.</p><figure><img src="'+r+'" alt="" width="80%" height="80%" tabindex="0" loading="lazy"><figcaption>KL-Divergence 수식 유도</figcaption></figure><p>수식은 <code>교차 엔트로피</code>의 기본 수식으로부터 유도되고 과정은 다음과 같다고 볼 수 있습니다.</p><ol><li><code>교차 엔트로피</code> 수식에 +- 값으로 <code>엔트로피</code> 관련 수식을 작성합니다.</li><li><code>엔트로피</code> 계산 간 사용되는 수식인 - 값이 존재하는 수식을 <code>H(P)</code>로 대체합니다.</li><li><code>교차 엔트로피</code> 수식과 남은 수식을 +, - 순서로 정렬하고 이를 요약합니다.</li><li>양 변에 <code>-H(P)</code>를 추가하여 우변에 존재하는 <code>H(P)</code> 값을 제거합니다.</li><li><code>H(P,Q)-H(P)</code>에 대한 수식이 유도됐고 해당 값이 <code>KL-Divergence</code> 입니다.</li></ol><h3 id="😲-수식-표현" tabindex="-1"><a class="header-anchor" href="#😲-수식-표현"><span>😲 수식 표현</span></a></h3><p><code>KL-Divergence</code>는 수식 표현 간 <code>KL(P||Q)</code>라 표현하고 아래처럼 수식 표현이 가능합니다.</p><figure><img src="'+n+'" alt="" width="60%" height="60%" tabindex="0" loading="lazy"><figcaption>KL-Divergence 수식</figcaption></figure><h3 id="🤔-사용-용도" tabindex="-1"><a class="header-anchor" href="#🤔-사용-용도"><span>🤔 사용 용도</span></a></h3><p>일반적으로 <code>교차 엔트로피</code>의 값을 작게 만들어 <code>H(P)</code>(실제 값)과 근접하게 만드는 것은<br> 다른 의미로는 <code>H(P)</code> 값은 고정된 값이므로 <code>KL-Divergence</code>의 값을 줄이는 것과 같습니다.</p><h3 id="😏-특성" tabindex="-1"><a class="header-anchor" href="#😏-특성"><span>😏 특성</span></a></h3><p><code>KL-Divergence</code>는 2가지의 특성이 대표적으로 특성은 아래와 같습니다.</p><ol><li><code>KL(P||Q)</code>의 값은 항상 0이상의 값을 갖습니다.</li><li><code>KL(P||Q)</code>와 <code>KL(Q||P)</code>의 값을 같지 않고 거리의 개념이 아닙니다.</li></ol><p>1번 특성의 경우 <code>H(P, Q)</code>의 값은 항상 <code>H(P)</code>보다 크기 때문에 성립됩니다.</p><p>2번 특성의 경우 <code>H(P, Q) != H(Q, P)</code>, <code>H(P) != H(Q)</code>라는 특성과 연관됩니다.<br> 해당 값이 다르기 때문에 <code>KL(P, Q) != KL(Q, P)</code>라는 특성이 성립되게 됩니다.</p><p>해당 부분은 <code>유클리디안 거리</code> 등과 달리 거리의 개념이 아님을 반증하고 있습니다.</p><hr><p>끝까지 읽어주셔서 감사드립니다. 😎</p>',23),l=[i];function p(d,g){return c(),t("div",null,l)}const h=e(a,[["render",p],["__file","KL-Divergence.html.vue"]]),v=JSON.parse('{"path":"/posts/Computing/AI/KL-Divergence.html","title":"[Artificial Intelligence] 확률분포의 차이를 계산하는 KL-Divergence","lang":"ko-KR","frontmatter":{"title":"[Artificial Intelligence] 확률분포의 차이를 계산하는 KL-Divergence","categories":["AI"],"tags":["AI","인공지능","불확실성","Uncertainty","무작위성","Randomness","확률","확률 변수","엔트로피","교차 엔트로피","Entropy","Cross Entropy","KL Divergence","Kullback-Leibler Divergence","Divergence","발산","차이","확률분포","정보 엔트로피","상대 엔트로피","정보 획득량","인포메이션 다이버전스","Information Divergence","손실함수","손실","Loss","Loss Function"],"date":"2024-11-18T00:00:00.000Z","order":203,"editLink":false,"lastUpdated":true,"description":"📈 KL(Kullback-Leibler) Divergence 이전 포스팅을 통해 불확실성을 측정하기 위한 엔트로피에 대한 개념을 알아봤습니다. KL-Divergence는 교차 엔트로피를 이용하여 확률분포의 차이를 계산하는 함수로 근사하는 다른 분포를 샘플링 시 발생할 수 있는 정보 엔트로피의 차이를 계산합니다. 여...","head":[["meta",{"property":"og:url","content":"https://blog.false.kr/posts/Computing/AI/KL-Divergence.html"}],["meta",{"property":"og:site_name","content":"찬스의 개발 블로그 : Chance Devlog"}],["meta",{"property":"og:title","content":"[Artificial Intelligence] 확률분포의 차이를 계산하는 KL-Divergence"}],["meta",{"property":"og:description","content":"📈 KL(Kullback-Leibler) Divergence 이전 포스팅을 통해 불확실성을 측정하기 위한 엔트로피에 대한 개념을 알아봤습니다. KL-Divergence는 교차 엔트로피를 이용하여 확률분포의 차이를 계산하는 함수로 근사하는 다른 분포를 샘플링 시 발생할 수 있는 정보 엔트로피의 차이를 계산합니다. 여..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://blog.false.kr/assets/image/Post/Computing/AI/KL-Divergence/1.png \\"KL-Divergence 수식 유도\\" =80%x80%"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"property":"og:updated_time","content":"2024-11-18T14:12:23.000Z"}],["meta",{"property":"article:author","content":"Chance"}],["meta",{"property":"article:tag","content":"AI"}],["meta",{"property":"article:tag","content":"인공지능"}],["meta",{"property":"article:tag","content":"불확실성"}],["meta",{"property":"article:tag","content":"Uncertainty"}],["meta",{"property":"article:tag","content":"무작위성"}],["meta",{"property":"article:tag","content":"Randomness"}],["meta",{"property":"article:tag","content":"확률"}],["meta",{"property":"article:tag","content":"확률 변수"}],["meta",{"property":"article:tag","content":"엔트로피"}],["meta",{"property":"article:tag","content":"교차 엔트로피"}],["meta",{"property":"article:tag","content":"Entropy"}],["meta",{"property":"article:tag","content":"Cross Entropy"}],["meta",{"property":"article:tag","content":"KL Divergence"}],["meta",{"property":"article:tag","content":"Kullback-Leibler Divergence"}],["meta",{"property":"article:tag","content":"Divergence"}],["meta",{"property":"article:tag","content":"발산"}],["meta",{"property":"article:tag","content":"차이"}],["meta",{"property":"article:tag","content":"확률분포"}],["meta",{"property":"article:tag","content":"정보 엔트로피"}],["meta",{"property":"article:tag","content":"상대 엔트로피"}],["meta",{"property":"article:tag","content":"정보 획득량"}],["meta",{"property":"article:tag","content":"인포메이션 다이버전스"}],["meta",{"property":"article:tag","content":"Information Divergence"}],["meta",{"property":"article:tag","content":"손실함수"}],["meta",{"property":"article:tag","content":"손실"}],["meta",{"property":"article:tag","content":"Loss"}],["meta",{"property":"article:tag","content":"Loss Function"}],["meta",{"property":"article:published_time","content":"2024-11-18T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-18T14:12:23.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"[Artificial Intelligence] 확률분포의 차이를 계산하는 KL-Divergence\\",\\"image\\":[\\"https://blog.false.kr/assets/image/Post/Computing/AI/KL-Divergence/1.png \\\\\\"KL-Divergence 수식 유도\\\\\\" =80%x80%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/KL-Divergence/2.png \\\\\\"KL-Divergence 수식\\\\\\" =60%x60%\\"],\\"datePublished\\":\\"2024-11-18T00:00:00.000Z\\",\\"dateModified\\":\\"2024-11-18T14:12:23.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Chance\\",\\"url\\":\\"https://blog.false.kr\\",\\"email\\":\\"chance0432@naver.com\\"}]}"]]},"headers":[{"level":2,"title":"📈 KL(Kullback-Leibler) Divergence","slug":"📈-kl-kullback-leibler-divergence","link":"#📈-kl-kullback-leibler-divergence","children":[{"level":3,"title":"🔢 수식 유도","slug":"🔢-수식-유도","link":"#🔢-수식-유도","children":[]},{"level":3,"title":"😲 수식 표현","slug":"😲-수식-표현","link":"#😲-수식-표현","children":[]},{"level":3,"title":"🤔 사용 용도","slug":"🤔-사용-용도","link":"#🤔-사용-용도","children":[]},{"level":3,"title":"😏 특성","slug":"😏-특성","link":"#😏-특성","children":[]}]}],"git":{"createdTime":1731938929000,"updatedTime":1731939143000,"contributors":[{"name":"Chance","email":"ahs0432@naver.com","commits":3}]},"readingTime":{"minutes":0.49,"words":146},"filePathRelative":"posts/Computing/AI/KL-Divergence.md","localizedDate":"2024년 11월 18일","excerpt":"<h2>📈 KL(Kullback-Leibler) Divergence</h2>\\n<p><a href=\\"/posts/Computing/AI/Cross-entropy\\">이전 포스팅</a>을 통해 <code>불확실성</code>을 측정하기 위한 <code>엔트로피</code>에 대한 개념을 알아봤습니다.</p>\\n<p><code>KL-Divergence</code>는 <code>교차 엔트로피</code>를 이용하여 <code>확률분포</code>의 차이를 계산하는 함수로<br>\\n근사하는 다른 <code>분포</code>를 샘플링 시 발생할 수 있는 <code>정보 엔트로피</code>의 차이를 계산합니다.</p>","autoDesc":true}');export{h as comp,v as data};
