import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,o as t,a as c}from"./app-zWBho-jB.js";const d="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/1.png",a="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/2.png",n="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/3.png",i="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/4.png",r="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/5.png",p="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/6.png",l="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/7.png",s="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/8.png",g="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/9.png",u="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/10.png",h="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/11.png",m="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/12.png",f="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/13.png",N="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/14.png",C="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/15.png",b="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/16.png",k="/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/17.png",I={},y=c('<h2 id="📷-합성곱-신경망-convolution-neural-network" tabindex="-1"><a class="header-anchor" href="#📷-합성곱-신경망-convolution-neural-network"><span>📷 합성곱 신경망(Convolution Neural Network)</span></a></h2><p><code>합성곱 신경망(Convolution Neural Network)</code>은 <code>합성곱</code>을 이용한 <code>신경망</code>으로<br> 이미지 처리 분야에서 공간에 대한 <code>특징 추출</code>에 유리하여 많이 사용되는 형태입니다.</p><p>이를 이용하여 <code>신호 처리</code>, <code>이미지 특징 분석</code>, <code>컴퓨터 비전</code> 등에서 사용됩니다.</p><h3 id="🔢-특징-맵-feature-map" tabindex="-1"><a class="header-anchor" href="#🔢-특징-맵-feature-map"><span>🔢 특징 맵(Feature Map)?</span></a></h3><p><code>특징 맵(Feature Map)</code>은 데이터가 <code>커널(Kernel)</code>이라는 <code>필터(Filter)</code>를 거치며,<br><code>선형 결합</code>을 통해 계산된 결과를 반환한 표 정도로 생각하는 것이 좋다고 생각됩니다.</p><p>여기서 <code>선형 결합</code>이란 <code>커널</code>을 이용하여 데이터의 요소를 곱한 값을 더하는 것입니다.</p><h4 id="_1️⃣-1차원-데이터" tabindex="-1"><a class="header-anchor" href="#_1️⃣-1차원-데이터"><span>1️⃣ 1차원 데이터</span></a></h4><p>예를 들어 1x8 크기의 <code>1차원 데이터</code>를 1x3 크기의 <code>커널</code>을 이용해 계산해볼 경우<br> 아래와 같이 연산이 수행되어 1x6 크기의 <code>특징 맵</code>을 반환하는 것이 확인됩니다.</p><figure><img src="'+d+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 특징 맵 도출 그림</figcaption></figure><p>알 수 있는 것은 <code>커널</code> 크기가 1x3이라면 <code>특징 맵</code>의 크기가 2개 줄었다는 것과<br><code>특징 맵</code>은 1x3인 경우 (-1, 0, 1) 데이터로 이뤄져서 만들어지는 것이 파악됩니다.</p><p>규칙으로 인지하신 분도 있겠지만 <code>커널</code>은 0을 기준으로 값의 대칭을 이루고 있으며,<br> 이 값의 대칭을 이루기 위해 <code>커널</code>의 크기는 3, 5, 7과 같은 홀수로 구성되고 있습니다.</p><p><code>1차원 데이터</code>를 <code>커널</code>을 통해 <code>특징 맵</code>을 도출하는 수식은 아래와 같이 구성됩니다.</p><figure><img src="'+a+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 특징 맵 도출 수식</figcaption></figure><p>여기서 <code>z</code>는 <code>입력된 데이터</code>를 <code>u</code>는 <code>커널</code>을, 마지막으로 <code>h</code>는 <code>커널</code>의 크기입니다.<br> 수식에 표에서 제공된 3이라는 <code>커널</code>의 크기를 대입하여 계산해보도록 하겠습니다.</p><figure><img src="'+n+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 특징 맵 도출 수식 풀이</figcaption></figure><p><code>f</code>가 1인 경우 0번, 1번, 2번에 각각 -1, 0, 1을 곱한 뒤 더하는 것이 확인됩니다.<br> 이렇게 될 시 <code>f</code>가 0이거나 예제 기준 7인 경우 계산이 성립되지 않는게 확인됩니다.</p><h4 id="_2️⃣-2차원-데이터" tabindex="-1"><a class="header-anchor" href="#_2️⃣-2차원-데이터"><span>2️⃣ 2차원 데이터</span></a></h4><p><code>1차원 데이터</code>에 대한 구조를 이해했다면 <code>2차원 데이터</code>에 대해서도 확인해보겠습니다.</p><p><code>2차원 데이터</code>는 8x8 형태이고 <code>커널</code>의 크기도 <code>2차원</code>의 형태로 3x3 크기로 구성하였습니다.</p><figure><img src="'+i+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>2차원 데이터 특징 맵 도출 그림</figcaption></figure><p>마찬가지로 확인 가능한 것은 <code>특징 맵</code>의 크기가 기존 <code>데이터</code>에 비해 작아졌다는 것과<br><code>커널</code>이 이전과 비슷하게 ((-1, 0, 1), (-1, 0, 1), (-1, 0, 1)) 형태로 생성됐다는 것입니다.</p><p>그렇다면 이러한 형태의 <code>커널</code>을 적용하는 수식은 어떻게 구성되는지 살펴보겠습니다.</p><figure><img src="'+r+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>2차원 데이터 특징 맵 도출 수식</figcaption></figure><p>기존 <code>1차원 데이터</code>에서 크게 변화하지는 않았고 수식에 추가만 된 형태가 확인됩니다.<br> 기존에는 <code>x</code> 축만 이용했기에 <code>x</code>만 있었지만 이제는 <code>y</code> 축의 값도 이용하고 있습니다.</p><p>해당 수식에 마찬가지로 <code>커널</code>의 크기인 <code>h</code> 값에 3을 대입하여 확인해보도록 하겠습니다.</p><figure><img src="'+p+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>2차원 데이터 특징 맵 도출 수식 풀이</figcaption></figure><p>추가로 제가 표현한 <code>커널</code>의 경우 오른쪽 값에서 왼쪽 값을 빼는 형태를 띄고 있습니다.<br> 해당 형태를 <code>수직 엣지</code> 라고 표현하고 이는 <code>수직</code> 데이터에 대한 <code>특성</code>을 추출합니다.</p><p>만약 흑백의 밝기를 이용하여 추출할 경우 오른쪽이 밝은 경우 값이 양수로 표현될 것이고,<br> 반대로 왼쪽이 밝거나 오른쪽이 비교적 어두운 경우 값이 작기 때문에 음수로 표현됩니다.</p><p>그렇기에 <code>수평</code> 형태 추출이 필요하고 위 값에서 아래 값을 빼는 형태를 만들 수 있습니다.<br> 이 경우 <code>수평 엣지</code>의 <code>커널</code>은 ((1, 1, 1), (0, 0, 0), (-1, -1, -1)) 형태를 띄는 걸 알 수 있습니다.</p><p>이런 형태를 이용해 <code>이미지</code>의 경우 색상 영역의 <code>RGB</code>를 포함한 다양한 <code>특성</code>을 추출합니다.</p><p>물론 이러한 <code>커널</code>도 하나의 <code>가중치</code>이므로 <code>신경망</code>의 학습 과정에서 값이 조정되게 됩니다.</p><p><a href="https://poloclub.github.io/cnn-explainer/" target="_blank" rel="noopener noreferrer">CNN Explainer</a> 페이지를 통해 <code>합성곱 신경망</code>에서 <code>커널</code>에 따른 <code>특징 맵</code> 확인이 가능합니다.</p><figure><img src="'+l+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>CNN Explainer</figcaption></figure><h3 id="🖼️-합성곱-층-convolution-layer" tabindex="-1"><a class="header-anchor" href="#🖼️-합성곱-층-convolution-layer"><span>🖼️ 합성곱 층(Convolution Layer)</span></a></h3><p>위에 설명한 <code>특징맵</code>을 추출하기 위한 과정이 <code>합성곱 층(Convolution Layer)</code>의 역할입니다.</p><p><code>합성곱 층</code>에서는 <code>출력 데이터</code>를 <code>유지</code>하거나 <code>축소</code>, 그리고 <code>바이어스</code> 값을 주기도 합니다.<br> 이번 파트에서는 각 방법에 대해서 하나하나 알아보고 계산 방법도 알아보도록 하겠습니다.</p><h4 id="💾-패딩-padding" tabindex="-1"><a class="header-anchor" href="#💾-패딩-padding"><span>💾 패딩(Padding)</span></a></h4><p><code>커널</code>의 구조 상 <code>커널</code>의 크기에 따라 <code>특징 맵</code>의 크기가 줄어든 것을 알 수 있습니다.</p><p>데이터의 입력이 부족한 결과로 <code>(h - 1) / 2</code>개만큼 각 축에 <code>특징 맵</code>을 축소시키게 됩니다.<br> 하지만, <code>커널</code>의 크기나 구성에 따라 <code>특징 맵</code>의 크기가 줄어들면 안되는 경우가 생기기도 합니다.</p><p>이런 상황에서 사용 가능한 것이 마치 입력이 더 커보이게 하는 <code>패딩(Padding)</code>이라는 방법입니다.</p><p>방법으로는 별도의 <code>패딩</code>을 적용하지 않아 <code>특징 맵</code>의 크기가 줄어드는 <code>유효(Vaild) 패딩</code>과<br><code>입력 데이터</code>의 크기와 <code>특징 맵</code>의 크기가 동일하게 유지되는 <code>동일(Same) 패딩</code>으로 나뉩니다.</p><p>상위 방식에 대해서는 별도의 설명은 하지 않으며, 채울 때의 방식 두 가지를 소개드리겠습니다.</p><h5 id="🫗-0-패딩-zero-padding" tabindex="-1"><a class="header-anchor" href="#🫗-0-패딩-zero-padding"><span>🫗 0 패딩(Zero Padding)</span></a></h5><p><code>0 패딩(Zero Padding)</code>은 부족한 데이터를 <code>0</code>으로 채우는 방법이라 볼 수 있습니다.<br> 해당 방식은 <code>0</code>을 추가하므로 추가된 <code>데이터</code>의 영향이 적어 일반적으로 많이 사용됩니다.</p><p><code>패딩</code>의 크기는 <code>p</code>라 표현되고 <code>1차원 데이터</code>에 <code>p</code>의 값을 1로 설정하면 아래와 같습니다.</p><figure><img src="'+s+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 0 패딩 예시</figcaption></figure><p><code>p</code>의 크기를 1로 설정했지만 양쪽에 <code>데이터</code>가 1개씩 추가된 것을 확인할 수 있습니다.<br><code>2차원 데이터</code>는 <code>p</code>의 값을 1로 설정할 경우 아래와 같은 형식으로 반영될 것입니다.</p><figure><img src="'+g+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>2차원 데이터 0 패딩 예시</figcaption></figure><p>2차원이라는 특징에 따라 상하좌우 모두 <code>데이터</code>가 1개씩 추가된 것을 확인할 수 있습니다.</p><h5 id="🖨️-복사-패딩-copy-padding" tabindex="-1"><a class="header-anchor" href="#🖨️-복사-패딩-copy-padding"><span>🖨️ 복사 패딩(Copy Padding)</span></a></h5><p><code>복사 패딩(Copy Padding)</code>은 근접한 <code>데이터</code>를 복사하는 형식으로 사용되는 방식입니다.</p><p><code>1차원 데이터</code>를 기준으로 <code>복사 패딩</code>을 적용할 경우 아래와 같이 수행될 수 있습니다.</p><figure><img src="'+u+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 복사 패딩 예시</figcaption></figure><p>아무래도 <code>데이터</code>가 <code>복사</code>되어 적용됐기 때문에 해당 값을 보존하는데 좋은 방법입니다.<br> 만약 <code>2차원 데이터</code>에 적용할 경우 아래와 같이 적용되는 것을 확인하실 수 있습니다.</p><figure><img src="'+h+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 복사 패딩 예시</figcaption></figure><p>기존 <code>0 패딩</code>과 달리 값들이 기존의 <code>특징</code>을 조금 더 갖고 있는 것이 확인됩니다.</p><h4 id="✂️-스트라이드-stride" tabindex="-1"><a class="header-anchor" href="#✂️-스트라이드-stride"><span>✂️ 스트라이드(Stride)</span></a></h4><p><code>스트라이드(Stride)</code>는 <code>데이터</code>에 <code>커널</code>을 적용할 때 이동거리를 의미한다고 볼 수 있습니다.</p><p>기본적으로 <code>스트라이드</code>는 <code>s</code>라는 값으로 표현하고 기본 상태의 경우 1의 값을 갖고 있습니다.<br> 1의 값을 갖는 경우 1칸씩 오른쪽으로 이동하는 것을 의미하고 이를 통해 <code>특징 맵</code>이 추출됩니다.</p><p>1인 상태에서 이동 형태를 그림으로 표현하면 아래와 같이 이동된다는 것을 알 수 있을 겁니다.</p><figure><img src="'+m+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 스트라이드 1 예시</figcaption></figure><p>이동할 때 한 칸씩 옆으로 이동하여 <code>커널</code>을 적용하고 <code>특징 맵</code>을 만들고 있는게 보입니다.<br> 그렇다면 <code>스트라이드</code> 값을 2를 준다면 어떻게 될지 그림으로 한 번 살펴보겠습니다.</p><figure><img src="'+f+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 스트라이드 2 예시</figcaption></figure><p><code>스트라이드</code>를 적용했더니 한 칸씩 이동하던 것이 두 칸씩 이동하는 효과가 적용됐습니다.<br> 이를 통해 기존 한 칸씩 이동하던 것에 비해 1/2로 <code>특징 맵</code>이 줄어든 것이 확인됩니다.</p><p><code>스트라이드</code>는 결국 <code>s</code>의 크기만큼 <code>특징 맵</code>의 최종 크기를 1/<code>s</code>로 만들게 됩니다.<br> 해당 개념은 이후 설명될 <code>풀링 층</code>의 개념에서도 사용되오니 참고 바랍니다.</p><h4 id="➕-바이어스-bias" tabindex="-1"><a class="header-anchor" href="#➕-바이어스-bias"><span>➕ 바이어스(Bias)</span></a></h4><p><code>바이어스</code>는 각 커널을 통과할 때의 기본적인 <code>가중값</code>을 제공하는 것을 의미합니다.<br> 그림으로 표현하면 아래와 같이 <code>커널</code> 통과 시 값이 그만큼 추가된 것이 확인됩니다.</p><figure><img src="'+N+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>1차원 데이터 바이어스 예시</figcaption></figure><h4 id="📚-커널의-크기" tabindex="-1"><a class="header-anchor" href="#📚-커널의-크기"><span>📚 커널의 크기</span></a></h4><p><code>커널</code>의 크기는 <code>h</code>로 표현하고 있고 <code>2차원 데이터</code>는 <code>h*h</code> 크기의 <code>커널</code>이 사용됩니다.</p><p>또한 <code>커널</code>은 입력 <code>데이터</code>의 <code>면</code> 그러니 <code>특징 맵</code>의 <code>채널</code>만큼 생성이 필요하게 됩니다.<br> 예를 들어 <code>RGB</code> 형태의 <code>데이터</code>를 입력 값으로 제공한 경우 아래와 같이 구성될 것입니다.</p><figure><img src="'+C+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>RGB 데이터 예시</figcaption></figure><p>위와 같이 <code>R, G, B</code>는 3개의 <code>채널</code>이 합쳐져 하나의 그림을 구성하는 형태로 볼 수 있습니다.<br> 이미지가 실제 의미하는 바를 파악하려면 3개의 <code>채널</code>에 <code>특징 맵</code>을 추출해야할 것입니다.</p><p>그렇다면 기본적으로 <code>커널</code>은 <code>h*h</code>의 크기로 구성되지만 앞서 제공된 <code>데이터</code>의 <code>채널</code>만큼<br> 단일 <code>커널</code>의 크기가 결정되고 <code>커널</code> 크기는 <code>h*h*k(입력 데이터의 채널)</code>로 구성됩니다.</p><figure><img src="'+b+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>RGB 데이터에 대한 커널 예시</figcaption></figure><p>여기서 알 수 있는건 <code>입력 데이터</code>, 앞 층의 <code>특징 맵</code>이 여러 <code>채널</code>이란 걸 파악할 수 있습니다.</p><p><code>특징 맵</code>이 여러 장이란 것은 앞 층에서 여러 개의 <code>커널</code>을 갖고 있다는 것을 의미하게 되며,<br> 이러한 <code>커널</code>의 개수는 <code>k</code>라는 형식으로 표현하고 현재 층의 개수를 <code>k&#39;</code>로 표현하게 됩니다.</p><p><code>k&#39;</code>의 개수는 다음 <code>합성곱 층</code>의 단일 <code>커널</code>의 <code>채널</code>과 동일하다는 것을 알 수 있습니다.</p><h4 id="️⃣-계산-방법" tabindex="-1"><a class="header-anchor" href="#️⃣-계산-방법"><span>#️⃣ 계산 방법</span></a></h4><p>위와 같이 <code>패딩</code>, <code>스트라이드</code>를 이용할 경우 <code>특징 맵</code>의 크기가 변경되는 것이 확인됩니다.<br> 또한 <code>커널</code>의 크기 뿐만 아니라 <code>개수</code>도 중요한 역할을 하고 있기에 명확한 계산이 필요합니다.</p><p>간단하게 여기서 한 가지 예시를 통해 계산하는 방법을 파악하고 <code>특징 맵</code>의 크기를 보겠습니다.</p><ul><li>파라미터 <ul><li><code>입력 데이터 (x*y*k)</code>: 512 * 512 * 3</li><li><code>커널 개수 (k&#39;)</code>: 128</li><li><code>커널 크기 (h)</code>: 5</li><li><code>패딩 (p)</code>: 1</li><li><code>스트라이드 (s)</code>: 2</li></ul></li><li>결과 <ul><li><code>커널</code> 크기: 5 * 5 * 3</li><li><code>특징 맵</code> 크기 <ul><li><code>패딩</code> 적용: (512 + (1*2)) * (512 + (1*2)) = 514 * 514</li><li><code>커널</code> 통과: 514 - ((5-1) / 2 * 2) * 514 - ((5-1) / 2 * 2) = 510 * 510</li><li><code>스트라이드</code> 적용: (510 / 2) * (510 / 2) = 255 * 255</li><li><code>커널 개수</code> 적용: 255 * 255 * 128</li></ul></li><li><code>가중치</code> 개수: 5 * 5 * 3 * 128 = 9,600개</li></ul></li></ul><p>상위 내용은 간단하게 기존의 계산 방식을 이용하여 현재 층의 <code>특징 맵</code>을 예상한 값입니다.<br> 이러한 계산 방식을 이용하면 다음 <code>입력 데이터</code>를 예상하여 <code>합성곱 층</code>을 구성 가능합니다.</p><h3 id="⏭️-풀링-층-pooling-layer" tabindex="-1"><a class="header-anchor" href="#⏭️-풀링-층-pooling-layer"><span>⏭️ 풀링 층(Pooling Layer)</span></a></h3><p><code>합성곱 층</code> 이후에 위치하는 층으로 <code>특징 맵</code>에 지나친 상세함을 줄이는 목적을 갖습니다.<br> 동작 방식은 <code>커널</code>을 거칠 때 값을 요약하고 이를 <code>특징 맵</code>에 반영하여 생성하는 형태입니다.</p><p>이전 설명한 <code>스트라이드</code>를 이용해 <code>커널 크기</code>에 맞춰 <code>특징 맵</code>의 크기를 줄이는 형식으로<br> 이렇게 진행 시 <code>특징 맵</code>의 크기를 줄여 이미지를 다운샘플링하는 것과 같은 효과를 보입니다.</p><figure><img src="'+k+'" alt="" width="70%" height="70%" tabindex="0" loading="lazy"><figcaption>최대 풀링 예시</figcaption></figure><p>일반적인 <code>풀링</code> 방식으로는 <code>풀링 커널</code> 내 가장 큰 값을 남기는 <code>최대 풀링 방식</code>과<br><code>풀링 커널</code> 내 값의 평균 값을 연산하여 남기게 되는 <code>평균 풀링 방식</code>이 사용됩니다.</p><p><code>합성곱 층</code>과의 차이는 <code>커널</code>에 별도의 <code>가중치</code>가 존재하지 않다라는 점이 존재하고,<br> 말 그대로 요약을 위한 층이기 때문에 <code>채널</code>의 크기에도 변동을 주지 않게 됩니다.</p><h3 id="💫-특징" tabindex="-1"><a class="header-anchor" href="#💫-특징"><span>💫 특징</span></a></h3><p>최종 특징으로는 <code>부분 연결성</code>과 <code>가중치 공유</code>라는 특성이 있다는 것을 확인할 수 있습니다.</p><p>기존 알고리즘들은 <code>완전 연결성</code>의 특징에 따라 <code>입력 노드</code>가 <code>은닉 노드</code>와 모두 연결됩니다.<br> 이러한 <code>완전 연결성</code>의 특징에 따라 각 <code>노드</code> 별 가중치를 별도로 만들어 관리하고 있습니다.</p><p>하지만 <code>합성곱 신경망</code>은 확인 시 <code>커널</code>이라는 <code>가중치</code>를 공유해서 사용한다는게 확인됩니다.<br> 또한 <code>커널</code>은 모든 <code>데이터</code>, 즉 <code>노드</code>와 연결되지 않기에 <code>부분 연결성</code>의 특징을 갖습니다.</p><p>이러한 특성을 이용하여 <code>가중치</code> 갱신에 소요되는 시간을 줄일 수 있다는 장점을 보여줍니다.</p><hr><p>다음 포스팅에서는 합성곱 신경망의 주요적인 구조와 예시를 알아보겠습니다.</p><p>정말 긴 글 끝까지 읽어주셔서 감사드립니다. 😎</p>',97),v=[y];function w(x,_){return t(),o("div",null,v)}const z=e(I,[["render",w],["__file","Convolution-Neural-Network-Intro.html.vue"]]),B=JSON.parse('{"path":"/posts/Computing/AI/Convolution-Neural-Network-Intro.html","title":"[Artificial Intelligence] 특징을 추출하는 합성곱 신경망의 기초","lang":"ko-KR","frontmatter":{"title":"[Artificial Intelligence] 특징을 추출하는 합성곱 신경망의 기초","categories":["AI"],"tags":["AI","인공지능","합성곱","Convolution","합성곱 신경망","Convolution Neural Network","신경망","Neural Network","레이어","층","Layer","커널","Kernel","필터","Filter","특징 맵","Feature Map","패딩","Padding","스트라이드","Stride","채널","Channel","이미지 인식","객체 인식","Object Detection"],"date":"2024-11-25T00:00:00.000Z","order":301,"editLink":false,"lastUpdated":true,"description":"📷 합성곱 신경망(Convolution Neural Network) 합성곱 신경망(Convolution Neural Network)은 합성곱을 이용한 신경망으로 이미지 처리 분야에서 공간에 대한 특징 추출에 유리하여 많이 사용되는 형태입니다. 이를 이용하여 신호 처리, 이미지 특징 분석, 컴퓨터 비전 등에서 사용됩...","head":[["meta",{"property":"og:url","content":"https://blog.false.kr/posts/Computing/AI/Convolution-Neural-Network-Intro.html"}],["meta",{"property":"og:site_name","content":"찬스의 개발 블로그 : Chance Devlog"}],["meta",{"property":"og:title","content":"[Artificial Intelligence] 특징을 추출하는 합성곱 신경망의 기초"}],["meta",{"property":"og:description","content":"📷 합성곱 신경망(Convolution Neural Network) 합성곱 신경망(Convolution Neural Network)은 합성곱을 이용한 신경망으로 이미지 처리 분야에서 공간에 대한 특징 추출에 유리하여 많이 사용되는 형태입니다. 이를 이용하여 신호 처리, 이미지 특징 분석, 컴퓨터 비전 등에서 사용됩..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/1.png \\"1차원 데이터 특징 맵 도출 그림\\" =70%x70%"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"property":"og:updated_time","content":"2024-11-25T13:15:56.000Z"}],["meta",{"property":"article:author","content":"Chance"}],["meta",{"property":"article:tag","content":"AI"}],["meta",{"property":"article:tag","content":"인공지능"}],["meta",{"property":"article:tag","content":"합성곱"}],["meta",{"property":"article:tag","content":"Convolution"}],["meta",{"property":"article:tag","content":"합성곱 신경망"}],["meta",{"property":"article:tag","content":"Convolution Neural Network"}],["meta",{"property":"article:tag","content":"신경망"}],["meta",{"property":"article:tag","content":"Neural Network"}],["meta",{"property":"article:tag","content":"레이어"}],["meta",{"property":"article:tag","content":"층"}],["meta",{"property":"article:tag","content":"Layer"}],["meta",{"property":"article:tag","content":"커널"}],["meta",{"property":"article:tag","content":"Kernel"}],["meta",{"property":"article:tag","content":"필터"}],["meta",{"property":"article:tag","content":"Filter"}],["meta",{"property":"article:tag","content":"특징 맵"}],["meta",{"property":"article:tag","content":"Feature Map"}],["meta",{"property":"article:tag","content":"패딩"}],["meta",{"property":"article:tag","content":"Padding"}],["meta",{"property":"article:tag","content":"스트라이드"}],["meta",{"property":"article:tag","content":"Stride"}],["meta",{"property":"article:tag","content":"채널"}],["meta",{"property":"article:tag","content":"Channel"}],["meta",{"property":"article:tag","content":"이미지 인식"}],["meta",{"property":"article:tag","content":"객체 인식"}],["meta",{"property":"article:tag","content":"Object Detection"}],["meta",{"property":"article:published_time","content":"2024-11-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-25T13:15:56.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"[Artificial Intelligence] 특징을 추출하는 합성곱 신경망의 기초\\",\\"image\\":[\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/1.png \\\\\\"1차원 데이터 특징 맵 도출 그림\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/2.png \\\\\\"1차원 데이터 특징 맵 도출 수식\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/3.png \\\\\\"1차원 데이터 특징 맵 도출 수식 풀이\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/4.png \\\\\\"2차원 데이터 특징 맵 도출 그림\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/5.png \\\\\\"2차원 데이터 특징 맵 도출 수식\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/6.png \\\\\\"2차원 데이터 특징 맵 도출 수식 풀이\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/7.png \\\\\\"CNN Explainer\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/8.png \\\\\\"1차원 데이터 0 패딩 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/9.png \\\\\\"2차원 데이터 0 패딩 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/10.png \\\\\\"1차원 데이터 복사 패딩 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/11.png \\\\\\"1차원 데이터 복사 패딩 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/12.png \\\\\\"1차원 데이터 스트라이드 1 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/13.png \\\\\\"1차원 데이터 스트라이드 2 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/14.png \\\\\\"1차원 데이터 바이어스 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/15.png \\\\\\"RGB 데이터 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/16.png \\\\\\"RGB 데이터에 대한 커널 예시\\\\\\" =70%x70%\\",\\"https://blog.false.kr/assets/image/Post/Computing/AI/Convolution-Neural-Network-Intro/17.png \\\\\\"최대 풀링 예시\\\\\\" =70%x70%\\"],\\"datePublished\\":\\"2024-11-25T00:00:00.000Z\\",\\"dateModified\\":\\"2024-11-25T13:15:56.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Chance\\",\\"url\\":\\"https://blog.false.kr\\",\\"email\\":\\"chance0432@naver.com\\"}]}"]]},"headers":[{"level":2,"title":"📷 합성곱 신경망(Convolution Neural Network)","slug":"📷-합성곱-신경망-convolution-neural-network","link":"#📷-합성곱-신경망-convolution-neural-network","children":[{"level":3,"title":"🔢 특징 맵(Feature Map)?","slug":"🔢-특징-맵-feature-map","link":"#🔢-특징-맵-feature-map","children":[]},{"level":3,"title":"🖼️ 합성곱 층(Convolution Layer)","slug":"🖼️-합성곱-층-convolution-layer","link":"#🖼️-합성곱-층-convolution-layer","children":[]},{"level":3,"title":"⏭️ 풀링 층(Pooling Layer)","slug":"⏭️-풀링-층-pooling-layer","link":"#⏭️-풀링-층-pooling-layer","children":[]},{"level":3,"title":"💫 특징","slug":"💫-특징","link":"#💫-특징","children":[]}]}],"git":{"createdTime":1732540535000,"updatedTime":1732540556000,"contributors":[{"name":"Chance","email":"ahs0432@naver.com","commits":2}]},"readingTime":{"minutes":1.51,"words":452},"filePathRelative":"posts/Computing/AI/Convolution-Neural-Network-Intro.md","localizedDate":"2024년 11월 25일","excerpt":"<h2>📷 합성곱 신경망(Convolution Neural Network)</h2>\\n<p><code>합성곱 신경망(Convolution Neural Network)</code>은 <code>합성곱</code>을 이용한 <code>신경망</code>으로<br>\\n이미지 처리 분야에서 공간에 대한 <code>특징 추출</code>에 유리하여 많이 사용되는 형태입니다.</p>\\n<p>이를 이용하여 <code>신호 처리</code>, <code>이미지 특징 분석</code>, <code>컴퓨터 비전</code> 등에서 사용됩니다.</p>\\n<h3>🔢 특징 맵(Feature Map)?</h3>","autoDesc":true}');export{z as comp,B as data};
